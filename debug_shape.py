import torch
from model.HTR_VT import create_model


def test_shape_alignment():
    print(">>> ğŸš€ Starting Shape Dry Run...")

    # æ£€æµ‹è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"â„¹ï¸ Using device: {device}")

    # 1. é…ç½®å‚æ•° (æ¨¡æ‹Ÿ Phase 2 é…ç½®)
    nb_cls = 80  # å­—ç¬¦é›†å¤§å°
    img_size = [512, 64]  # [W, H] - ä¸ option.py ä¸­çš„æ ¼å¼ä¸€è‡´
    max_len = 26  # ABINet åˆ†æ”¯çš„æœ€å¤§é¢„æµ‹é•¿åº¦
    bs = 2  # æ¨¡æ‹Ÿ Batch Size

    # 2. åˆå§‹åŒ–æ¨¡å‹ (å¯ç”¨ Language Model åˆ†æ”¯)
    # æ³¨æ„: create_model æœŸæœ› img_size ä¸º [H, W] æ ¼å¼ï¼Œæ‰€ä»¥éœ€è¦åè½¬
    # è¿™ä¸ train.py ä¸­çš„ img_size[::-1] ä¸€è‡´
    try:
        model = create_model(
            nb_cls=nb_cls,
            img_size=img_size[::-1],  # [W, H] -> [H, W] = [64, 512]
            use_language_model=True,  # <--- å…³é”®å¼€å…³
            max_length=max_len,
        ).to(device)
        print("âœ… Model initialized successfully.")
    except Exception as e:
        print(f"âŒ Model Init Failed: {e}")
        import traceback

        traceback.print_exc()
        return

    # 3. æ„é€ ä¼ªé€ æ•°æ®
    # è¾“å…¥: (B, C, H, W) -> HTR-VT æ¥å— grayscale (1 channel) æˆ– RGB (3)
    # æ³¨æ„: dataset.py é‡Œé€šå¸¸æ˜¯ (1, 64, 512)
    dummy_input = torch.randn(bs, 1, 64, 512).to(device)
    print(f"â„¹ï¸ Input Shape: {dummy_input.shape}")

    # 4. å‰å‘ä¼ æ’­æµ‹è¯•
    try:
        # å…ˆæµ‹è¯•å„ä¸ªå­æ¨¡å—çš„è¾“å‡ºå½¢çŠ¶
        print("\n--- Debug: Testing submodules ---")

        with torch.no_grad():
            # Test ResNet + ViT encoder
            x = model.layer_norm(dummy_input)
            x = model.patch_embed(x)
            print(f"After patch_embed (ResNet): {x.shape}")

            b, c, h, w = x.shape
            x = x.view(b, c, -1).permute(0, 2, 1)
            print(f"After reshape to sequence: {x.shape}")

            x = x + model.pos_embed
            for i, blk in enumerate(model.blocks):
                x = blk(x)
            x = model.norm(x)
            print(f"After ViT blocks (visual_feat): {x.shape}")

            # Test CTC head
            ctc_logits = model.head(x)
            ctc_logits = model.layer_norm(ctc_logits)
            print(f"CTC logits: {ctc_logits.shape}")

            # Test Position Attention
            print("\n--- Debug: Testing PositionAttention ---")
            attn_vecs, attn_scores = model.pos_attn(x)
            print(f"PositionAttention output: {attn_vecs.shape}")

            # Test visual classifier
            vis_logits = model.vis_cls(attn_vecs)
            print(f"Visual logits: {vis_logits.shape}")

            # Test Language Model
            print("\n--- Debug: Testing BCNLanguage ---")
            vis_probs = torch.softmax(vis_logits, dim=-1)
            lang_output = model.language_model(vis_probs)
            lang_logits = lang_output["logits"]
            print(f"Language logits: {lang_logits.shape}")

        print("\n--- Full Forward Pass ---")
        output = model(dummy_input)

        # 5. æ£€æŸ¥è¾“å‡ºç»“æ„
        if isinstance(output, dict):
            ctc_out = output.get("ctc")
            attn_out = output.get("attn")
            print("âœ… Forward pass returned a dictionary.")
        else:
            print(f"âŒ Forward pass returned {type(output)}, expected dict.")
            return

        # 6. æ£€æŸ¥ CTC åˆ†æ”¯ç»´åº¦
        # é¢„æœŸ: (B, 128, nb_cls)
        if ctc_out is not None:
            print(
                f"âœ… CTC Output Shape: {ctc_out.shape} (Expected: [{bs}, 128, {nb_cls}])"
            )
        else:
            print("âŒ CTC Output is None!")

        # 7. æ£€æŸ¥ ABINet åˆ†æ”¯ç»´åº¦ (æœ€å…³é”®!)
        # é¢„æœŸ: (B, max_len, nb_cls) -> (2, 26, 80)
        if attn_out is not None:
            if attn_out.shape == (bs, max_len, nb_cls):
                print(
                    f"âœ… Attention Output Shape: {attn_out.shape} MATCHES Expected: [{bs}, {max_len}, {nb_cls}]"
                )
            else:
                print(
                    f"âŒ Attention Output Shape Mismatch! Got {attn_out.shape}, Expected [{bs}, {max_len}, {nb_cls}]"
                )
        else:
            print("âŒ Attention Output is None! Check 'use_language_model' flag.")

    except RuntimeError as e:
        print(f"âŒ Runtime Error during forward: {e}")
        import traceback

        traceback.print_exc()
        # å¸¸è§é”™è¯¯æç¤º: "mat1 and mat2 shapes cannot be multiplied" é€šå¸¸æ„å‘³ç€ Linear å±‚è¾“å…¥ä¸å¯¹


if __name__ == "__main__":
    test_shape_alignment()
